# ArXiv RAG System üìö

A Retrieval-Augmented Generation (RAG) system for querying academic papers from the ArXiv dataset. This system uses Milvus for vector storage, SPECTER for embeddings, and supports multiple LLM providers (Gemini, OpenAI, HuggingFace) via a unified service interface.

## üèóÔ∏è Architecture

### **High-Level Flow**

```text

Query Flow:
+------------+       +-------------------+       +----------------+
| User Query | ----> | Embedding Service | ----> | Milvus Service |
+------------+       +-------------------+       +-------+--------+
                                                         |
                                                         | Search (Cosine)
                                                         v
                                                +--------------------+
                                                | Semantic Retriever |
                                                +--------+-----------+
                                                         |
                                                         | Top-k Indices
                                                         v
   +--------------+      Lookup Context         +-------------+
   | Metadata CSV | <-------------------------> | LLM Service |
   +--------------+                             +------+------+
                                                       |
                                                       | Prompt + Context
                                                       v
                                               +----------------+
                                               |  LLM Provider  |
                                               +-------+--------+
                                                       |
                                                       | Response
                                                       v
                                                    +------+
                                                    | User |
                                                    +------+
```

### **Core Services (`src/`)**

| Service | File Path | Role |
|---------|-----------|------|
| **LLM Service** | `src/generation/llm_service.py` | **Unified Interface** for LLMs. Handles context building, prompt construction, and dispatching to Gemini/OpenAI/HF clients. |
| **RAG Pipeline** | `src/generation/milvus_rag_pipeline.py` | **Orchestrator**. Connects the Retriever, Embedder, and LLM Service to execute the RAG flow. |
| **Milvus Service** | `src/vector_db/milvus_service.py` | **Vector DB**. Manages the Milvus database, handles schema, indexing, and vector insertion/search. |
| **Retriever** | `src/retrieval/retriever.py` | **Search Logic**. Uses Milvus Client to find relevant document indices based on query embeddings. |
| **Embedder** | `src/embedding/embedding_service.py` | **Vectorization**. Uses `allenai-specter` to convert text queries and documents into 768-dim vectors. |

---

## üìÇ Project Structure

```
‚îú‚îÄ‚îÄ main.py                     # üöÄ Main entry point (CLI / Interactive)
‚îú‚îÄ‚îÄ prepare_arxiv_data.py       # üõ†Ô∏è Step 1: Data Prep Script
‚îú‚îÄ‚îÄ generate_embeddings.py      # üõ†Ô∏è Step 2: Embedding Script
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Place arxiv-metadata-oai-snapshot.json here
‚îÇ   ‚îú‚îÄ‚îÄ csv_files/              # Generated CSV chunks (for processing)
‚îÇ   ‚îú‚îÄ‚îÄ processed/              # metadata_all.csv (for runtime lookup)
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/             # Generated .npy embedding files
‚îÇ   ‚îî‚îÄ‚îÄ milvus/                 # Milvus database file (sqlite/local)
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ data_ingestion/         # Data ingestion to Milvus
    ‚îú‚îÄ‚îÄ generation/             # LLM logic & Pipeline
    ‚îú‚îÄ‚îÄ retrieval/              # Search logic
    ‚îú‚îÄ‚îÄ vector_db/              # Database connection
    ‚îî‚îÄ‚îÄ embedding/              # Model loading
```

---

## ‚ö° Setup Guide (From Scratch)

Follow these steps in order to set up the system.

### 1. Prerequisites
- Python 3.9+
- [ArXiv Metadata Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv) (download `arxiv-metadata-oai-snapshot.json`)
- API Keys for one or more providers:
    - Google Gemini (`GEMINI_API_KEY`)
    - OpenAI (`OPENAI_API_KEY`)
    - Hugging Face (`HUGGINGFACE_API_KEY`)

### 2. Installation
```bash
# Create virtual env
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt
```

### 3. Configuration
Create a `.env` file in the root directory:
```ini
GEMINI_API_KEY=your_key_here
OPENAI_API_KEY=your_key_here
HUGGINGFACE_API_KEY=your_key_here
```

### 4. Prepare ArXiv Data
Convert the raw JSON into processed CSVs:
```bash
# Place arxiv-metadata-oai-snapshot.json in data/raw/ first!
python prepare_arxiv_data.py --json-path data/raw/arxiv-metadata-oai-snapshot.json --chunk-size 80000
```
**Output:** 
- `data/csv_files/metadata_*.csv` - Chunks for embedding generation
- `data/processed/metadata_all.csv` - Combined file for runtime lookup

### 5. Generate Embeddings
Convert papers into 768-dim vectors using SPECTER:
```bash
python generate_embeddings.py --csv data/csv_files/metadata_0_80000.csv
```
**Output:** `data/embeddings/embeddings_0_80000.npy`

>  Takes ~30-60 min on T4 GPU for 80k papers on google colab. Repeat for additional chunks if needed.

### 6. Ingest Data into Milvus
Load embeddings into the vector database:
```bash
python -c "from src.data_ingestion.ingest_to_milvus import ingest_to_milvus; ingest_to_milvus('data/embeddings/embeddings_0_80000.npy', 'data/csv_files/metadata_0_80000.csv', 'data/milvus/arxiv_papers.db', 'arxiv_papers')"
```
**OR** run `main.py` and it will auto-ingest on first run if data is missing.

### 7. Run the Application üöÄ
```bash
# Interactive mode (default)
python main.py

# Specify LLM provider
python main.py --provider openai

# Skip data checks for faster startup
python main.py --skip-check
```

Ask questions like:
```
üí¨ What are transformer models?
üí¨ Explain attention mechanisms
```

---

## üõ†Ô∏è Unified LLM Service
The system uses a factory pattern in `src/generation/llm_service.py` to switch between providers seamlessly.

**Usage:**
```python
from src.generation.llm_service import LLMService

# Initialize with provider
llm = LLMService(provider="gemini") 
# OR provider="openai", provider="huggingface"

# Generate
response = llm.generate_response(query, context_documents)
print(response.content)
```